{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## module 1,2,3 combined"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will record continuos audio and transcribe them it the set of 10 second audio clips until an stop event occure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Lenovo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Lenovo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OSError encountered: [Errno -9996] Invalid input device (no default output device)\n"
     ]
    }
   ],
   "source": [
    "import pyaudio\n",
    "import wave\n",
    "import speech_recognition as sr\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import Counter\n",
    "from transformers import BertTokenizer, BertModel, BartForConditionalGeneration, BartTokenizer\n",
    "import torch\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "# Download necessary NLTK data\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Parameters\n",
    "FORMAT = pyaudio.paInt16  # Audio format\n",
    "CHANNELS = 1  # Number of channels\n",
    "RATE = 44100  # Sample rate (Hz)\n",
    "CHUNK = 1024  # Chunk size (number of frames per buffer)\n",
    "RECORD_SECONDS = 10  # Duration of the recording (seconds)\n",
    "OUTPUT_FILENAME = \"recorded_audio.wav\"  # Output filename\n",
    "\n",
    "try:\n",
    "    # Initialize PyAudio\n",
    "    audio = pyaudio.PyAudio()\n",
    "\n",
    "    # Open stream\n",
    "    stream = audio.open(format=FORMAT, channels=CHANNELS,\n",
    "                        rate=RATE, input=True,\n",
    "                        frames_per_buffer=CHUNK)\n",
    "\n",
    "    print(\"Recording...\")\n",
    "\n",
    "    frames = []\n",
    "\n",
    "    # Record data\n",
    "    for i in range(0, int(RATE / CHUNK * RECORD_SECONDS)):\n",
    "        data = stream.read(CHUNK)\n",
    "        frames.append(data)\n",
    "\n",
    "    print(\"Recording finished.\")\n",
    "\n",
    "    # Stop and close the stream\n",
    "    stream.stop_stream()\n",
    "    stream.close()\n",
    "    audio.terminate()\n",
    "\n",
    "    # Save the recorded data as a WAV file\n",
    "    with wave.open(OUTPUT_FILENAME, 'wb') as wf:\n",
    "        wf.setnchannels(CHANNELS)\n",
    "        wf.setsampwidth(audio.get_sample_size(FORMAT))\n",
    "        wf.setframerate(RATE)\n",
    "        wf.writeframes(b''.join(frames))\n",
    "\n",
    "    print(f\"Audio recorded and saved as {OUTPUT_FILENAME}\")\n",
    "\n",
    "    # Speech recognition\n",
    "    r = sr.Recognizer()\n",
    "    with sr.AudioFile(OUTPUT_FILENAME) as source:\n",
    "        audio_data = r.record(source)  # Read the entire audio file\n",
    "\n",
    "        try:\n",
    "            # Recognize the speech using Google Web Speech API\n",
    "            text = r.recognize_google(audio_data)\n",
    "            print(\"Transcription: \" + text)\n",
    "            # Append the transcription to a text file\n",
    "            with open(\"transcription.txt\", \"a\") as f:\n",
    "                f.write(text + \"\\n\")\n",
    "            # Save the transcription to a text file\n",
    "            with open(\"transcription.txt\", \"w\") as f:\n",
    "                f.write(text)\n",
    "                \n",
    "        except sr.UnknownValueError:\n",
    "            print(\"Google Speech Recognition could not understand the audio\")\n",
    "        except sr.RequestError as e:\n",
    "            print(\"Could not request results from Google Speech Recognition service; {0}\".format(e))\n",
    "\n",
    "    # Text processing\n",
    "    with open(\"transcription.txt\", \"r\") as file:\n",
    "        text = file.read()\n",
    "\n",
    "    # Tokenize the text\n",
    "    words = word_tokenize(text)\n",
    "\n",
    "    # Remove punctuation and make lowercase\n",
    "    words = [word.lower() for word in words if word.isalnum()]\n",
    "\n",
    "    # Remove stop words\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    filtered_words = [word for word in words if word not in stop_words]\n",
    "\n",
    "    # Count the frequency of each word\n",
    "    word_freq = Counter(filtered_words)\n",
    "\n",
    "    # Select the top N keywords (you can adjust N as needed)\n",
    "    N = 10\n",
    "    keywords = word_freq.most_common(N)\n",
    "\n",
    "    # Print the keywords\n",
    "    print(\"Top keywords:\")\n",
    "    for keyword, freq in keywords:\n",
    "        print(f\"{keyword}: {freq}\")\n",
    "\n",
    "    # Save the keywords to a text file\n",
    "    with open(\"keywords.txt\", \"w\") as file:\n",
    "        for keyword, freq in keywords:\n",
    "            file.write(f\"{keyword}\\n\")\n",
    "\n",
    "    # Process keywords with BERT\n",
    "    # Load pre-trained BERT model and tokenizer\n",
    "    bert_model = BertModel.from_pretrained('bert-base-uncased')\n",
    "    bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "    # Loading the dataset of keywords\n",
    "    datapath = r\"C:\\Users\\Lenovo\\Documents\\Rohit_AI_ML\\SummariseIT\\dataset.csv\"\n",
    "    df = pd.read_csv(datapath)\n",
    "    # Flatten the dataset to create a set of valid keywords\n",
    "    valid_keywords = set()\n",
    "    for column in df.columns:\n",
    "        valid_keywords.update(df[column].dropna().str.strip().tolist())\n",
    "\n",
    "    def extract_keywords_from_tokens(text, model, tokenizer, num_keywords=5):\n",
    "        # Tokenize input\n",
    "        inputs = tokenizer(text, return_tensors='pt')\n",
    "        \n",
    "        # Get embeddings\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "            last_hidden_states = outputs.last_hidden_state\n",
    "\n",
    "        # Convert token IDs to tokens\n",
    "        tokens = tokenizer.convert_ids_to_tokens(inputs['input_ids'][0])\n",
    "        \n",
    "        # Get the [CLS] token's embedding\n",
    "        cls_embedding = last_hidden_states[:, 0, :].squeeze()\n",
    "        \n",
    "        # Calculate similarity between each token embedding and the [CLS] embedding\n",
    "        similarities = torch.matmul(last_hidden_states.squeeze(), cls_embedding)\n",
    "        \n",
    "        # Get the indices of the top-n tokens with the highest similarity\n",
    "        top_indices = similarities.topk(num_keywords).indices\n",
    "\n",
    "        # Extract the corresponding tokens, excluding [CLS] and checking if they are in valid_keywords\n",
    "        keywords = [tokens[i] for i in top_indices if tokens[i] != '[CLS]' and tokens[i] in valid_keywords]\n",
    "        \n",
    "        return keywords\n",
    "\n",
    "    # Read and process input text\n",
    "    file_path = \"keywords.txt\"\n",
    "    with open(file_path, \"r\") as file:\n",
    "        text = file.read()\n",
    "\n",
    "    # Use the function to extract keywords\n",
    "    extracted_keywords = extract_keywords_from_tokens(text, bert_model, bert_tokenizer)\n",
    "\n",
    "    # Print extracted keywords\n",
    "    print(\"Extracted keywords:\")\n",
    "    for idx, keyword in enumerate(extracted_keywords, start=1):\n",
    "        print(f\"Keyword {idx}: {keyword}\")\n",
    "\n",
    "        # Search the web for the keyword on Wikipedia\n",
    "        search_url = f\"https://en.wikipedia.org/wiki/{keyword}\"\n",
    "        headers = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3\"}\n",
    "        response = requests.get(search_url, headers=headers)\n",
    "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "        # Extract the relevant information from the search result\n",
    "        paragraphs = soup.find_all(\"p\")\n",
    "        extracted_text = \"\"\n",
    "        for paragraph in paragraphs:\n",
    "            extracted_text += paragraph.get_text() + \" \"\n",
    "        extracted_text = extracted_text.strip()\n",
    "\n",
    "        # Generate a summary using BART\n",
    "        bart_model = BartForConditionalGeneration.from_pretrained('facebook/bart-large-cnn')\n",
    "        bart_tokenizer = BartTokenizer.from_pretrained('facebook/bart-large-cnn')\n",
    "\n",
    "        def generate_summary(text, model, tokenizer):\n",
    "            inputs = tokenizer(text, return_tensors='pt', padding=True, truncation=True, max_length=1024)\n",
    "            summary_ids = model.generate(inputs['input_ids'], num_beams=4, max_length=150, early_stopping=True)\n",
    "            summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "            return summary\n",
    "\n",
    "        # Generate summary\n",
    "        summary = generate_summary(extracted_text, bart_model, bart_tokenizer)\n",
    "        print(f\"Summary of {keyword}:\")\n",
    "        print(summary)\n",
    "\n",
    "except OSError as e:\n",
    "    print(f\"OSError encountered: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to nltk_resources...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "selected index k out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 190\u001b[0m\n\u001b[0;32m    187\u001b[0m     text \u001b[38;5;241m=\u001b[39m file\u001b[38;5;241m.\u001b[39mread()\n\u001b[0;32m    189\u001b[0m \u001b[38;5;66;03m# Use the function to extract keywords\u001b[39;00m\n\u001b[1;32m--> 190\u001b[0m keywords \u001b[38;5;241m=\u001b[39m \u001b[43mextract_keywords_from_tokens\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    192\u001b[0m \u001b[38;5;66;03m# Print extracted keywords\u001b[39;00m\n\u001b[0;32m    193\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExtracted keywords:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[2], line 177\u001b[0m, in \u001b[0;36mextract_keywords_from_tokens\u001b[1;34m(text, model, tokenizer, num_keywords)\u001b[0m\n\u001b[0;32m    174\u001b[0m similarities \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmatmul(last_hidden_states\u001b[38;5;241m.\u001b[39msqueeze(), cls_embedding)\n\u001b[0;32m    176\u001b[0m \u001b[38;5;66;03m# Get the indices of the top-n tokens with the highest similarity\u001b[39;00m\n\u001b[1;32m--> 177\u001b[0m top_indices \u001b[38;5;241m=\u001b[39m \u001b[43msimilarities\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtopk\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_keywords\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mindices\n\u001b[0;32m    179\u001b[0m \u001b[38;5;66;03m# Extract the corresponding tokens, excluding [CLS] and checking if they are in valid_keywords\u001b[39;00m\n\u001b[0;32m    180\u001b[0m keywords \u001b[38;5;241m=\u001b[39m [tokens[i] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m top_indices \u001b[38;5;28;01mif\u001b[39;00m tokens[i] \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m[CLS]\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m tokens[i] \u001b[38;5;129;01min\u001b[39;00m valid_keywords]\n",
      "\u001b[1;31mRuntimeError\u001b[0m: selected index k out of range"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pyaudio\n",
    "import wave\n",
    "import speech_recognition as sr\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import Counter\n",
    "from transformers import BertTokenizer, BertModel, BartForConditionalGeneration, BartTokenizer\n",
    "import torch\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import gradio as gr\n",
    "import pyttsx3\n",
    "import re\n",
    "\n",
    "# Define custom NLTK data path\n",
    "NLTK_CUSTOM_PATH = 'nltk_resources'\n",
    "\n",
    "# Ensure the folder exists\n",
    "os.makedirs(NLTK_CUSTOM_PATH, exist_ok=True)\n",
    "\n",
    "# Add the custom path to NLTK's search locations\n",
    "nltk.data.path.append(NLTK_CUSTOM_PATH)\n",
    "\n",
    "# Function to check if an NLTK resource exists\n",
    "def is_resource_available(resource_path):\n",
    "    try:\n",
    "        nltk.data.find(resource_path)\n",
    "        return True\n",
    "    except LookupError:\n",
    "        return False\n",
    "\n",
    "# Download missing NLTK resources\n",
    "for resource in ['punkt', 'punkt_tab', 'stopwords']:\n",
    "    if not is_resource_available(f'tokenizers/{resource}'):\n",
    "        nltk.download(resource, download_dir=NLTK_CUSTOM_PATH)\n",
    "\n",
    "# audio recording parameters removed \n",
    "\n",
    "# Function to record audio and convert it into a text file\n",
    "def record_audio():\n",
    "    # copied from module 1\n",
    "    ### Initializing The Recognizer\n",
    "    r = sr.Recognizer()\n",
    "    # Parameters\n",
    "    FORMAT = pyaudio.paInt16  # Audio format\n",
    "    CHANNELS = 1  # Number of channels\n",
    "    RATE = 44100  # Sample rate (Hz)\n",
    "    CHUNK = 1024  # Chunk size (number of frames per buffer)\n",
    "    RECORD_SECONDS = 10  # Duration of the recording (seconds)\n",
    "    OUTPUT_FILENAME = \"recorded_audio.wav\"  # Output filename\n",
    "\n",
    "    try:\n",
    "        # Initialize PyAudio\n",
    "        audio = pyaudio.PyAudio()\n",
    "\n",
    "        # Open stream\n",
    "        stream = audio.open(format=FORMAT, channels=CHANNELS,\n",
    "                            rate=RATE, input=True,\n",
    "                            frames_per_buffer=CHUNK)\n",
    "\n",
    "        frames = []\n",
    "\n",
    "        # Record data\n",
    "        for i in range(0, int(RATE / CHUNK * RECORD_SECONDS)):\n",
    "            data = stream.read(CHUNK)\n",
    "            frames.append(data)\n",
    "\n",
    "        # Stop and close the stream\n",
    "        stream.stop_stream()\n",
    "        stream.close()\n",
    "        audio.terminate()\n",
    "\n",
    "        # Save the recorded data as a WAV file\n",
    "        with wave.open(OUTPUT_FILENAME, 'wb') as wf:\n",
    "            wf.setnchannels(CHANNELS)\n",
    "            wf.setsampwidth(audio.get_sample_size(FORMAT))\n",
    "            wf.setframerate(RATE)\n",
    "            wf.writeframes(b''.join(frames))\n",
    "\n",
    "        print(f\"Audio recorded and saved as {OUTPUT_FILENAME}\")\n",
    "        with sr.AudioFile(OUTPUT_FILENAME) as source:\n",
    "            audio = r.record(source)  # Read the entire audio file\n",
    "\n",
    "            try:\n",
    "                # Recognize the speech using Google Web Speech API\n",
    "                text = r.recognize_google(audio)\n",
    "                print(\"Transcription: \" + text)\n",
    "                # Append the transcription to a text file\n",
    "                with open(\"transcription.txt\", \"a\") as f:\n",
    "                    f.write(text + \"\\n\")\n",
    "                # Save the transcription to a text file\n",
    "                with open(\"transcription.txt\", \"w\") as f:\n",
    "                    f.write(text)\n",
    "                    \n",
    "            except sr.UnknownValueError:\n",
    "                print(\"Google Speech Recognition could not understand the audio\")\n",
    "            except sr.RequestError as e:\n",
    "                print(\"Could not request results from Google Speech Recognition service; {0}\".format(e))\n",
    "    except OSError as e:\n",
    "        print(f\"OSError encountered: {e}\")\n",
    "#speech recognition function was removed and combined above \n",
    "\n",
    "# Extract keywords from text using tokenization \n",
    "#module 2 used here\n",
    "def extract_keywords(text, num_keywords=10):\n",
    "    \"\"\"Extracts top keywords from transcribed text using NLTK.\"\"\"\n",
    "\n",
    "    # Read the text file\n",
    "    with open(\"transcription.txt\", \"r\") as file:\n",
    "        text = file.read()\n",
    "\n",
    "    # Tokenize the text\n",
    "    words = word_tokenize(text)\n",
    "\n",
    "    # Remove punctuation and make lowercase\n",
    "    words = [word.lower() for word in words if word.isalnum()]\n",
    "\n",
    "    # Remove stop words\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    filtered_words = [word for word in words if word not in stop_words]\n",
    "\n",
    "    # Count the frequency of each word\n",
    "    word_freq = Counter(filtered_words)\n",
    "\n",
    "    # Select the top N keywords (you can adjust N as needed)\n",
    "    N = 10\n",
    "    keywords = word_freq.most_common(N)\n",
    "\n",
    "    # Print the keywords\n",
    "    # print(\"Top keywords:\")\n",
    "    # for keyword, freq in keywords:\n",
    "    #     print(f\"{keyword}: {freq}\")\n",
    "\n",
    "    # Save the keywords to a text file\n",
    "    with open(\"keywords.txt\", \"w\") as file:\n",
    "        for keyword, freq in keywords:\n",
    "            file.write(f\"{keyword}\\n\")\n",
    "\n",
    "# Function to extract keywords from 'transcription.txt' and save them to 'keywords.txt'\n",
    "# is removed and merged above \n",
    "\n",
    "\n",
    "# Load pre-trained model and tokenizer\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Loading the dataset of keywords\n",
    "datapath = r\"dataset.csv\"\n",
    "df = pd.read_csv(datapath)\n",
    "# Flatten the dataset to create a set of valid keywords\n",
    "valid_keywords = set()\n",
    "for column in df.columns:\n",
    "    valid_keywords.update(df[column].dropna().str.strip().tolist())\n",
    "\n",
    "def extract_keywords_from_tokens(text, model, tokenizer, num_keywords=5):\n",
    "    # Tokenize input\n",
    "    inputs = tokenizer(text, return_tensors='pt')\n",
    "    \n",
    "    # Get embeddings\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        last_hidden_states = outputs.last_hidden_state\n",
    "\n",
    "    # Convert token IDs to tokens\n",
    "    tokens = tokenizer.convert_ids_to_tokens(inputs['input_ids'][0])\n",
    "    \n",
    "    # Get the [CLS] token's embedding\n",
    "    cls_embedding = last_hidden_states[:, 0, :].squeeze()\n",
    "    \n",
    "    # Calculate similarity between each token embedding and the [CLS] embedding\n",
    "    similarities = torch.matmul(last_hidden_states.squeeze(), cls_embedding)\n",
    "    \n",
    "    # Get the indices of the top-n tokens with the highest similarity\n",
    "    top_indices = similarities.topk(num_keywords).indices\n",
    "\n",
    "    # Extract the corresponding tokens, excluding [CLS] and checking if they are in valid_keywords\n",
    "    keywords = [tokens[i] for i in top_indices if tokens[i] != '[CLS]' and tokens[i] in valid_keywords]\n",
    "    \n",
    "    return keywords\n",
    "\n",
    "# Read and process input text\n",
    "file_path = \"keywords.txt\"\n",
    "with open(file_path, \"r\") as file:\n",
    "    text = file.read()\n",
    "\n",
    "# Use the function to extract keywords\n",
    "keywords = extract_keywords_from_tokens(text, model, tokenizer)\n",
    "\n",
    "# Print extracted keywords\n",
    "print(\"Extracted keywords:\")\n",
    "for idx, keyword in enumerate(keywords, start=1):\n",
    "    print(f\"Keyword {idx}: {keyword}\")\n",
    "\n",
    "# Summarization using BART\n",
    "bart_model = BartForConditionalGeneration.from_pretrained('facebook/bart-large-cnn')\n",
    "bart_tokenizer = BartTokenizer.from_pretrained('facebook/bart-large-cnn')\n",
    "\n",
    "def generate_summary(keyword):\n",
    "    \"\"\"Fetches Wikipedia content for a keyword and summarizes it using BART.\"\"\"\n",
    "    search_url = f\"https://en.wikipedia.org/wiki/{keyword.replace(' ', '_')}\"\n",
    "    headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
    "    response = requests.get(search_url, headers=headers)\n",
    "    \n",
    "    if response.status_code != 200:\n",
    "        return f\"Could not find Wikipedia page for '{keyword}'\"\n",
    "    \n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "    paragraphs = soup.find_all(\"p\")\n",
    "    extracted_text = \" \".join([p.get_text() for p in paragraphs]).strip()\n",
    "\n",
    "    if not extracted_text:\n",
    "        return f\"No relevant Wikipedia content for '{keyword}'\"\n",
    "\n",
    "    inputs = bart_tokenizer(extracted_text, return_tensors='pt', padding=True, truncation=True, max_length=1024)\n",
    "    summary_ids = bart_model.generate(inputs['input_ids'], num_beams=4, max_length=150, early_stopping=True)\n",
    "    \n",
    "    return bart_tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "\n",
    "# Main function to process audio, extract keywords, and generate summaries\n",
    "def main_interface(audio_input):\n",
    "    \"\"\"Processes recorded audio: transcribes, extracts keywords, and summarizes them.\"\"\"\n",
    "    transcribed_text = transcribe_audio(audio_input)\n",
    "\n",
    "    if \"could not\" in transcribed_text.lower():\n",
    "        return transcribed_text\n",
    "\n",
    "    extract_keywords_from_file(\"transcription.txt\", \"keywords.txt\", num_keywords=10)\n",
    "\n",
    "    with open(\"keywords.txt\", \"r\") as file:\n",
    "        keyword_text = file.read().strip()\n",
    "\n",
    "    refined_keywords = extract_keywords_with_bert(keyword_text, bert_model, bert_tokenizer)\n",
    "\n",
    "    if not refined_keywords:\n",
    "        return \"No valid keywords found in the transcription.\"\n",
    "\n",
    "    summaries = {keyword: generate_summary(keyword) for keyword in refined_keywords}\n",
    "\n",
    "    result = f\"**Transcribed Text:**\\n{transcribed_text}\\n\\n**Summaries:**\\n\"\n",
    "    for keyword, summary in summaries.items():\n",
    "        result += f\"- **{keyword.capitalize()}**: {summary}\\n\\n\"\n",
    "\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to nltk_resources...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recording...\n",
      "Recording complete.\n",
      "Recording...\n",
      "Recording complete.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pyaudio\n",
    "import wave\n",
    "import speech_recognition as sr\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import Counter\n",
    "from transformers import BertTokenizer, BertModel, BartForConditionalGeneration, BartTokenizer\n",
    "import torch\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import gradio as gr\n",
    "import pyttsx3\n",
    "import re\n",
    "\n",
    "# Define custom NLTK data path\n",
    "NLTK_CUSTOM_PATH = 'nltk_resources'\n",
    "\n",
    "# Ensure the folder exists\n",
    "os.makedirs(NLTK_CUSTOM_PATH, exist_ok=True)\n",
    "\n",
    "# Add the custom path to NLTK's search locations\n",
    "nltk.data.path.append(NLTK_CUSTOM_PATH)\n",
    "\n",
    "# Function to check if an NLTK resource exists\n",
    "def is_resource_available(resource_path):\n",
    "    try:\n",
    "        nltk.data.find(resource_path)\n",
    "        return True\n",
    "    except LookupError:\n",
    "        return False\n",
    "\n",
    "# Download missing NLTK resources\n",
    "for resource in ['punkt', 'stopwords']:\n",
    "    if not is_resource_available(f'tokenizers/{resource}'):\n",
    "        nltk.download(resource, download_dir=NLTK_CUSTOM_PATH)\n",
    "\n",
    "# Function to record audio\n",
    "def record_audio(output_filename=\"recorded_audio.wav\", duration=10, sample_rate=44100):\n",
    "    \"\"\"Record audio and save to a WAV file.\"\"\"\n",
    "    audio = pyaudio.PyAudio()\n",
    "    stream = audio.open(format=pyaudio.paInt16, channels=1, rate=sample_rate, input=True, frames_per_buffer=1024)\n",
    "\n",
    "    frames = []\n",
    "    print(\"Recording...\")\n",
    "\n",
    "    for _ in range(int(sample_rate / 1024 * duration)):\n",
    "        frames.append(stream.read(1024))\n",
    "\n",
    "    print(\"Recording complete.\")\n",
    "\n",
    "    stream.stop_stream()\n",
    "    stream.close()\n",
    "    audio.terminate()\n",
    "\n",
    "    with wave.open(output_filename, 'wb') as wf:\n",
    "        wf.setnchannels(1)\n",
    "        wf.setsampwidth(audio.get_sample_size(pyaudio.paInt16))\n",
    "        wf.setframerate(sample_rate)\n",
    "        wf.writeframes(b''.join(frames))\n",
    "\n",
    "    return output_filename\n",
    "\n",
    "# Function to transcribe recorded audio\n",
    "def transcribe_audio(audio_filename):\n",
    "    \"\"\"Convert speech to text using Google Speech Recognition.\"\"\"\n",
    "    recognizer = sr.Recognizer()\n",
    "\n",
    "    if not os.path.exists(audio_filename):\n",
    "        return \"Invalid audio file path.\"\n",
    "\n",
    "    with sr.AudioFile(audio_filename) as source:\n",
    "        audio_data = recognizer.record(source)\n",
    "        try:\n",
    "            transcription = recognizer.recognize_google(audio_data)\n",
    "\n",
    "            with open(\"transcription.txt\", \"w\") as f:\n",
    "                f.write(transcription)\n",
    "\n",
    "            return transcription\n",
    "        except sr.UnknownValueError:\n",
    "            return \"Could not understand audio.\"\n",
    "        except sr.RequestError as e:\n",
    "            return f\"Request error: {e}\"\n",
    "\n",
    "# Function to extract keywords from transcribed text\n",
    "def extract_keywords_from_file(input_file, output_file, num_keywords=10):\n",
    "    \"\"\"Extracts keywords from a text file.\"\"\"\n",
    "    with open(input_file, \"r\") as file:\n",
    "        text = file.read()\n",
    "\n",
    "    words = word_tokenize(text.lower())\n",
    "    words = [word for word in words if word.isalnum() and word not in stopwords.words(\"english\")]\n",
    "\n",
    "    word_freq = Counter(words)\n",
    "    keywords = [word for word, _ in word_freq.most_common(num_keywords)]\n",
    "\n",
    "    with open(output_file, \"w\") as file:\n",
    "        file.write(\"\\n\".join(keywords))\n",
    "\n",
    "    return keywords\n",
    "\n",
    "# Load pre-trained BERT model and tokenizer\n",
    "bert_model = BertModel.from_pretrained('bert-base-uncased')\n",
    "bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Function to extract refined keywords using BERT\n",
    "def extract_keywords_with_bert(text, model, tokenizer, num_keywords=5):\n",
    "    \"\"\"Extracts keywords using BERT embeddings.\"\"\"\n",
    "    inputs = tokenizer(text, return_tensors='pt')\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        last_hidden_states = outputs.last_hidden_state\n",
    "\n",
    "    tokens = tokenizer.convert_ids_to_tokens(inputs['input_ids'][0])\n",
    "    similarities = torch.matmul(last_hidden_states.squeeze(), last_hidden_states[:, 0, :].squeeze())\n",
    "    \n",
    "    top_indices = similarities.topk(num_keywords).indices\n",
    "    keywords = [tokens[i] for i in top_indices if tokens[i].isalpha()]\n",
    "    \n",
    "    return keywords\n",
    "\n",
    "# Function to summarize extracted keywords\n",
    "bart_model = BartForConditionalGeneration.from_pretrained('facebook/bart-large-cnn')\n",
    "bart_tokenizer = BartTokenizer.from_pretrained('facebook/bart-large-cnn')\n",
    "\n",
    "def generate_summary(keyword):\n",
    "    \"\"\"Fetches Wikipedia content for a keyword and summarizes it using BART.\"\"\"\n",
    "    search_url = f\"https://en.wikipedia.org/wiki/{keyword.replace(' ', '_')}\"\n",
    "    headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
    "    response = requests.get(search_url, headers=headers)\n",
    "    \n",
    "    if response.status_code != 200:\n",
    "        return f\"Could not find Wikipedia page for '{keyword}'\"\n",
    "    \n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "    paragraphs = soup.find_all(\"p\")\n",
    "    extracted_text = \" \".join([p.get_text() for p in paragraphs]).strip()\n",
    "\n",
    "    if not extracted_text:\n",
    "        return f\"No relevant Wikipedia content for '{keyword}'\"\n",
    "\n",
    "    inputs = bart_tokenizer(extracted_text, return_tensors='pt', padding=True, truncation=True, max_length=1024)\n",
    "    summary_ids = bart_model.generate(inputs['input_ids'], num_beams=4, max_length=150, early_stopping=True)\n",
    "    \n",
    "    return bart_tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "\n",
    "# Main function to record audio, extract keywords, and generate summaries\n",
    "def main_interface():\n",
    "    \"\"\"Runs full process: recording, transcribing, extracting keywords, summarizing.\"\"\"\n",
    "    \n",
    "    audio_path = record_audio()\n",
    "    \n",
    "    transcribed_text = transcribe_audio(audio_path)\n",
    "\n",
    "    if \"could not\" in transcribed_text.lower():\n",
    "        return transcribed_text\n",
    "\n",
    "    extract_keywords_from_file(\"transcription.txt\", \"keywords.txt\", num_keywords=10)\n",
    "\n",
    "    with open(\"keywords.txt\", \"r\") as file:\n",
    "        keyword_text = file.read().strip()\n",
    "\n",
    "    refined_keywords = extract_keywords_with_bert(keyword_text, bert_model, bert_tokenizer)\n",
    "\n",
    "    if not refined_keywords:\n",
    "        return \"No valid keywords found.\"\n",
    "\n",
    "    summaries = {keyword: generate_summary(keyword) for keyword in refined_keywords}\n",
    "\n",
    "    result = f\"**Transcribed Text:**\\n{transcribed_text}\\n\\n**Summaries:**\\n\"\n",
    "    for keyword, summary in summaries.items():\n",
    "        result += f\"- **{keyword.capitalize()}**: {summary}\\n\\n\"\n",
    "\n",
    "    return result\n",
    "\n",
    "# âœ… Gradio Interface\n",
    "iface = gr.Interface(fn=main_interface, inputs=None, outputs=\"text\", title=\"SummariseIT\", description=\"Records, transcribes, extracts keywords, and summarizes.\")\n",
    "\n",
    "iface.launch()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recording...\n",
      "Recording finished.\n",
      "Audio recorded and saved as recorded_audio.wav\n",
      "Google Speech Recognition could not understand the audio\n",
      "Top keywords:\n",
      "instagram: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\tilak\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\tilak\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.2731,  0.0477,  0.0559,  ..., -0.4933,  0.2429,  0.4301],\n",
      "         [ 0.4000, -0.5024,  0.2372,  ..., -0.3041,  0.7607,  0.1820],\n",
      "         [ 0.4276, -0.3590,  0.0432,  ..., -0.2957, -0.6141, -0.6569],\n",
      "         [ 0.9027,  0.0675, -0.0979,  ...,  0.1569, -0.6991, -0.2826]]])\n",
      "Extracted keywords:\n",
      "No keywords found.\n"
     ]
    }
   ],
   "source": [
    "import speech_recognition as sr\n",
    "import pyttsx3\n",
    "r = sr.Recognizer()\n",
    "import pyaudio\n",
    "import wave\n",
    "import speech_recognition as sr\n",
    "import pyttsx3\n",
    "### Initializing The Recognizer\n",
    "r = sr.Recognizer()\n",
    "# Parameters\n",
    "FORMAT = pyaudio.paInt16  # Audio format\n",
    "CHANNELS = 1  # Number of channels\n",
    "RATE = 44100  # Sample rate (Hz)\n",
    "CHUNK = 1024  # Chunk size (number of frames per buffer)\n",
    "RECORD_SECONDS = 10  # Duration of the recording (seconds)\n",
    "OUTPUT_FILENAME = \"recorded_audio.wav\"  # Output filename\n",
    "\n",
    "try:\n",
    "    # Initialize PyAudio\n",
    "    audio = pyaudio.PyAudio()\n",
    "\n",
    "    # Open stream\n",
    "    stream = audio.open(format=FORMAT, channels=CHANNELS,\n",
    "                        rate=RATE, input=True,\n",
    "                        frames_per_buffer=CHUNK)\n",
    "\n",
    "    print(\"Recording...\")\n",
    "\n",
    "    frames = []\n",
    "\n",
    "    # Record data\n",
    "    for i in range(0, int(RATE / CHUNK * RECORD_SECONDS)):\n",
    "        data = stream.read(CHUNK)\n",
    "        frames.append(data)\n",
    "\n",
    "    print(\"Recording finished.\")\n",
    "\n",
    "    # Stop and close the stream\n",
    "    stream.stop_stream()\n",
    "    stream.close()\n",
    "    audio.terminate()\n",
    "\n",
    "    # Save the recorded data as a WAV file\n",
    "    with wave.open(OUTPUT_FILENAME, 'wb') as wf:\n",
    "        wf.setnchannels(CHANNELS)\n",
    "        wf.setsampwidth(audio.get_sample_size(FORMAT))\n",
    "        wf.setframerate(RATE)\n",
    "        wf.writeframes(b''.join(frames))\n",
    "\n",
    "    print(f\"Audio recorded and saved as {OUTPUT_FILENAME}\")\n",
    "    with sr.AudioFile(OUTPUT_FILENAME) as source:\n",
    "        audio = r.record(source)  # Read the entire audio file\n",
    "\n",
    "        try:\n",
    "            # Recognize the speech using Google Web Speech API\n",
    "            text = r.recognize_google(audio)\n",
    "            print(\"Transcription: \" + text)\n",
    "            # Append the transcription to a text file\n",
    "            with open(\"transcription.txt\", \"a\") as f:\n",
    "                f.write(text + \"\\n\")\n",
    "            # Save the transcription to a text file\n",
    "            with open(\"transcription.txt\", \"w\") as f:\n",
    "                f.write(text)\n",
    "                \n",
    "        except sr.UnknownValueError:\n",
    "            print(\"Google Speech Recognition could not understand the audio\")\n",
    "        except sr.RequestError as e:\n",
    "            print(\"Could not request results from Google Speech Recognition service; {0}\".format(e))\n",
    "except OSError as e:\n",
    "    print(f\"OSError encountered: {e}\")\n",
    "import nltk\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('stopwords')\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import Counter\n",
    "\n",
    "# Read the text file\n",
    "with open(\"transcription.txt\", \"r\") as file:\n",
    "    text = file.read()\n",
    "\n",
    "# Tokenize the text\n",
    "words = word_tokenize(text)\n",
    "\n",
    "# Remove punctuation and make lowercase\n",
    "words = [word.lower() for word in words if word.isalnum()]\n",
    "\n",
    "# Remove stop words\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "filtered_words = [word for word in words if word not in stop_words]\n",
    "\n",
    "# Count the frequency of each word\n",
    "word_freq = Counter(filtered_words)\n",
    "\n",
    "# Select the top N keywords (you can adjust N as needed)\n",
    "N = 10\n",
    "keywords = word_freq.most_common(N)\n",
    "\n",
    "# Print the keywords\n",
    "print(\"Top keywords:\")\n",
    "for keyword, freq in keywords:\n",
    "    print(f\"{keyword}: {freq}\")\n",
    "\n",
    "# Save the keywords to a text file\n",
    "with open(\"keywords.txt\", \"w\") as file:\n",
    "    for keyword, freq in keywords:\n",
    "        file.write(f\"{keyword}\\n\")\n",
    "\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "\n",
    "# Load pre-trained model and tokenizer\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Read and process input text\n",
    "file_path = \"keywords.txt\"\n",
    "with open(file_path, \"r\") as file:\n",
    "    text = file.read()\n",
    "\n",
    "# Tokenize input\n",
    "inputs = tokenizer(text, return_tensors='pt')\n",
    "\n",
    "# Get embeddings\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "    last_hidden_states = outputs.last_hidden_state\n",
    "\n",
    "print(last_hidden_states)\n",
    "\n",
    "import torch\n",
    "import pandas as pd\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "# Load pre-trained model and tokenizer\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Loading the dataset of keywords\n",
    "datapath = r\"dataset.csv\"\n",
    "df = pd.read_csv(datapath)\n",
    "# Flatten the dataset to create a set of valid keywords\n",
    "valid_keywords = set()\n",
    "for column in df.columns:\n",
    "    valid_keywords.update(df[column].dropna().str.strip().tolist())\n",
    "\n",
    "def extract_keywords_from_tokens(text, model, tokenizer, num_keywords=5):\n",
    "    # Tokenize input\n",
    "    inputs = tokenizer(text, return_tensors='pt')\n",
    "    \n",
    "    # Get embeddings\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        last_hidden_states = outputs.last_hidden_state\n",
    "\n",
    "    # Convert token IDs to tokens\n",
    "    tokens = tokenizer.convert_ids_to_tokens(inputs['input_ids'][0])\n",
    "    \n",
    "    # Get the [CLS] token's embedding\n",
    "    cls_embedding = last_hidden_states[:, 0, :].squeeze()\n",
    "    \n",
    "    # Calculate similarity between each token embedding and the [CLS] embedding\n",
    "    similarities = torch.matmul(last_hidden_states.squeeze(), cls_embedding)\n",
    "    \n",
    "    # Get the indices of the top-n tokens with the highest similarity\n",
    "    top_indices = similarities.topk(min(num_keywords, len(similarities))).indices\n",
    "\n",
    "    # Extract the corresponding tokens, excluding [CLS] and checking if they are in valid_keywords\n",
    "    keywords = [tokens[i] for i in top_indices if tokens[i] != '[CLS]' and tokens[i] in valid_keywords]\n",
    "    \n",
    "    return keywords\n",
    "\n",
    "# Read and process input text\n",
    "file_path = \"keywords.txt\"\n",
    "with open(file_path, \"r\") as file:\n",
    "    text = file.read()\n",
    "\n",
    "# Use the function to extract keywords\n",
    "keywords = extract_keywords_from_tokens(text, model, tokenizer)\n",
    "\n",
    "# Print extracted keywords\n",
    "print(\"Extracted keywords:\")\n",
    "for idx, keyword in enumerate(keywords, start=1):\n",
    "    print(f\"Keyword {idx}: {keyword}\")\n",
    "\n",
    "import torch\n",
    "import pandas as pd\n",
    "from transformers import BertTokenizer, BertModel, BartForConditionalGeneration, BartTokenizer\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Load pre-trained BERT model and tokenizer\n",
    "bert_model = BertModel.from_pretrained('bert-base-uncased')\n",
    "bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Load pre-trained BART model and tokenizer for summarization\n",
    "bart_model = BartForConditionalGeneration.from_pretrained('facebook/bart-large-cnn')\n",
    "bart_tokenizer = BartTokenizer.from_pretrained('facebook/bart-large-cnn')\n",
    "\n",
    "# Loading the dataset of keywords\n",
    "datapath = r\"dataset.csv\"\n",
    "df = pd.read_csv(datapath)\n",
    "# Flatten the dataset to create a set of valid keywords\n",
    "valid_keywords = set()\n",
    "for column in df.columns:\n",
    "    valid_keywords.update(df[column].dropna().str.strip().tolist())\n",
    "\n",
    "def extract_keywords_from_tokens(text, model, tokenizer, num_keywords=5):\n",
    "    # Tokenize input\n",
    "    inputs = tokenizer(text, return_tensors='pt')\n",
    "    \n",
    "    # Get embeddings\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        last_hidden_state = outputs.last_hidden_state\n",
    "\n",
    "    # Convert token IDs to tokens\n",
    "    tokens = tokenizer.convert_ids_to_tokens(inputs['input_ids'][0])\n",
    "    \n",
    "    # Get the [CLS] token's embedding\n",
    "    cls_embedding = last_hidden_state[:, 0, :].squeeze()\n",
    "    \n",
    "    # Calculate similarity between each token embedding and the [CLS] embedding\n",
    "    similarities = torch.matmul(last_hidden_state.squeeze(), cls_embedding)\n",
    "    \n",
    "    # Get the indices of the top-n tokens with the highest similarity\n",
    "    top_indices = similarities.topk(num_keywords).indices\n",
    "\n",
    "    # Extract the corresponding tokens, excluding [CLS] and checking if they are in valid_keywords\n",
    "    keywords = [tokens[i] for i in top_indices if tokens[i] != '[CLS]' and tokens[i] in valid_keywords]\n",
    "    \n",
    "    return keywords\n",
    "\n",
    "# Read and process input text\n",
    "file_path = \"keywords.txt\"\n",
    "with open(file_path, \"r\") as file:\n",
    "    text = file.read()\n",
    "\n",
    "# Use the function to extract keywords\n",
    "def extract_keywords_from_tokens(text, model, tokenizer, num_keywords=5):\n",
    "    # Tokenize input\n",
    "    inputs = tokenizer(text, return_tensors='pt')\n",
    "\n",
    "    # Get embeddings\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        last_hidden_state = outputs.last_hidden_state\n",
    "\n",
    "    # Convert token IDs to tokens\n",
    "    tokens = tokenizer.convert_ids_to_tokens(inputs['input_ids'][0])\n",
    "\n",
    "    # Get the [CLS] token's embedding\n",
    "    cls_embedding = last_hidden_state[:, 0, :].squeeze()\n",
    "\n",
    "    # Calculate similarity between each token embedding and the [CLS] embedding\n",
    "    similarities = torch.matmul(last_hidden_state.squeeze(), cls_embedding)\n",
    "\n",
    "    # Get the indices of the top-n tokens with the highest similarity\n",
    "    top_indices = similarities.topk(min(num_keywords, len(similarities))).indices\n",
    "\n",
    "    # Extract the corresponding tokens, excluding [CLS] and checking if they are in valid_keywords\n",
    "    keywords = [tokens[i] for i in top_indices if tokens[i] != '[CLS]' and tokens[i] in valid_keywords]\n",
    "    \n",
    "    return keywords\n",
    "\n",
    "# Read and process input text\n",
    "file_path = \"keywords.txt\"\n",
    "with open(file_path, \"r\") as file:\n",
    "    text = file.read().strip()\n",
    "\n",
    "# Use the function to extract keywords\n",
    "keywords = extract_keywords_from_tokens(text, bert_model, bert_tokenizer)\n",
    "\n",
    "# Check if keywords were extracted\n",
    "if keywords:\n",
    "    print(\"Extracted keywords:\")\n",
    "    for idx, keyword in enumerate(keywords, start=1):\n",
    "        print(f\"Keyword {idx}: {keyword}\")\n",
    "\n",
    "        # Search the web for the keyword on Wikipedia\n",
    "        search_url = f\"https://en.wikipedia.org/wiki/{keyword}\"\n",
    "        headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
    "        response = requests.get(search_url, headers=headers)\n",
    "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "        # Extract the relevant information from the search result\n",
    "        paragraphs = soup.find_all(\"p\")\n",
    "        extracted_text = \" \".join([p.get_text() for p in paragraphs]).strip()\n",
    "\n",
    "        # Generate summary using BART\n",
    "        def generate_summary(text, model, tokenizer):\n",
    "            inputs = tokenizer(text, return_tensors='pt', padding=True, truncation=True, max_length=1024)\n",
    "            summary_ids = model.generate(inputs['input_ids'], num_beams=4, max_length=150, early_stopping=True)\n",
    "            return tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "\n",
    "        # Generate summary\n",
    "        summary = generate_summary(extracted_text, bart_model, bart_tokenizer)\n",
    "        print(f\"Summary of {keyword}: {summary}\")\n",
    "else:\n",
    "    print(\"No keywords found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
