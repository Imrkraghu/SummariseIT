{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using BERT for Keyword Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BERT (Bidirectional Encoder Representations from Transformers) has revolutionized the way we approach natural language processing tasks, including keyword extraction. Its ability to understand context and semantics makes it particularly effective. Here’s how BERT enhances keyword extraction:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Contextual Understanding: BERT’s bidirectional training allows it to grasp the context of words in a sentence, leading to more accurate keyword identification.\n",
    "* Fine-tuning: By fine-tuning BERT on specific datasets, we can improve its performance in extracting relevant keywords tailored to particular domains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\tilak\\OneDrive\\Documents\\summarizeit\\SummariseIT\\.venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.2731,  0.0477,  0.0559,  ..., -0.4933,  0.2429,  0.4301],\n",
      "         [ 0.4000, -0.5024,  0.2372,  ..., -0.3041,  0.7607,  0.1820],\n",
      "         [ 0.4276, -0.3590,  0.0432,  ..., -0.2957, -0.6141, -0.6569],\n",
      "         [ 0.9027,  0.0675, -0.0979,  ...,  0.1569, -0.6991, -0.2826]]])\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "\n",
    "# Load pre-trained model and tokenizer\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Read and process input text\n",
    "file_path = \"keywords.txt\"\n",
    "with open(file_path, \"r\") as file:\n",
    "    text = file.read()\n",
    "\n",
    "# Tokenize input\n",
    "inputs = tokenizer(text, return_tensors='pt')\n",
    "\n",
    "# Get embeddings\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "    last_hidden_states = outputs.last_hidden_state\n",
    "\n",
    "print(last_hidden_states)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted keywords:\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "# Load pre-trained model and tokenizer\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Loading the dataset of keywords\n",
    "datapath = r\"dataset.csv\"\n",
    "df = pd.read_csv(datapath)\n",
    "# Flatten the dataset to create a set of valid keywords\n",
    "valid_keywords = set()\n",
    "for column in df.columns:\n",
    "    valid_keywords.update(df[column].dropna().str.strip().tolist())\n",
    "\n",
    "def extract_keywords_from_tokens(text, model, tokenizer, num_keywords=5):\n",
    "    # Tokenize input\n",
    "    inputs = tokenizer(text, return_tensors='pt')\n",
    "    \n",
    "    # Get embeddings\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        last_hidden_states = outputs.last_hidden_state\n",
    "\n",
    "    # Convert token IDs to tokens\n",
    "    tokens = tokenizer.convert_ids_to_tokens(inputs['input_ids'][0])\n",
    "    \n",
    "    # Get the [CLS] token's embedding\n",
    "    cls_embedding = last_hidden_states[:, 0, :].squeeze()\n",
    "    \n",
    "    # Calculate similarity between each token embedding and the [CLS] embedding\n",
    "    similarities = torch.matmul(last_hidden_states.squeeze(), cls_embedding)\n",
    "    \n",
    "    # Get the indices of the top-n tokens with the highest similarity\n",
    "    top_indices = similarities.topk(min(num_keywords, len(similarities))).indices\n",
    "\n",
    "    # Extract the corresponding tokens, excluding [CLS] and checking if they are in valid_keywords\n",
    "    keywords = [tokens[i] for i in top_indices if tokens[i] != '[CLS]' and tokens[i] in valid_keywords]\n",
    "    \n",
    "    return keywords\n",
    "\n",
    "# Read and process input text\n",
    "file_path = \"keywords.txt\"\n",
    "with open(file_path, \"r\") as file:\n",
    "    text = file.read()\n",
    "\n",
    "# Use the function to extract keywords\n",
    "keywords = extract_keywords_from_tokens(text, model, tokenizer)\n",
    "\n",
    "# Print extracted keywords\n",
    "print(\"Extracted keywords:\")\n",
    "for idx, keyword in enumerate(keywords, start=1):\n",
    "    print(f\"Keyword {idx}: {keyword}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Web scraping the results of keywords from the web"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "selected index k out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 55\u001b[0m\n\u001b[0;32m     52\u001b[0m     text \u001b[38;5;241m=\u001b[39m file\u001b[38;5;241m.\u001b[39mread()\n\u001b[0;32m     54\u001b[0m \u001b[38;5;66;03m# Use the function to extract keywords\u001b[39;00m\n\u001b[1;32m---> 55\u001b[0m keywords \u001b[38;5;241m=\u001b[39m \u001b[43mextract_keywords_from_tokens\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbert_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbert_tokenizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;66;03m# Print extracted keywords\u001b[39;00m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExtracted keywords:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[5], line 42\u001b[0m, in \u001b[0;36mextract_keywords_from_tokens\u001b[1;34m(text, model, tokenizer, num_keywords)\u001b[0m\n\u001b[0;32m     39\u001b[0m similarities \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmatmul(last_hidden_state\u001b[38;5;241m.\u001b[39msqueeze(), cls_embedding)\n\u001b[0;32m     41\u001b[0m \u001b[38;5;66;03m# Get the indices of the top-n tokens with the highest similarity\u001b[39;00m\n\u001b[1;32m---> 42\u001b[0m top_indices \u001b[38;5;241m=\u001b[39m \u001b[43msimilarities\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtopk\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_keywords\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mindices\n\u001b[0;32m     44\u001b[0m \u001b[38;5;66;03m# Extract the corresponding tokens, excluding [CLS] and checking if they are in valid_keywords\u001b[39;00m\n\u001b[0;32m     45\u001b[0m keywords \u001b[38;5;241m=\u001b[39m [tokens[i] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m top_indices \u001b[38;5;28;01mif\u001b[39;00m tokens[i] \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m[CLS]\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m tokens[i] \u001b[38;5;129;01min\u001b[39;00m valid_keywords]\n",
      "\u001b[1;31mRuntimeError\u001b[0m: selected index k out of range"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from transformers import BertTokenizer, BertModel, BartForConditionalGeneration, BartTokenizer\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Load pre-trained BERT model and tokenizer\n",
    "bert_model = BertModel.from_pretrained('bert-base-uncased')\n",
    "bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Load pre-trained BART model and tokenizer for summarization\n",
    "bart_model = BartForConditionalGeneration.from_pretrained('facebook/bart-large-cnn')\n",
    "bart_tokenizer = BartTokenizer.from_pretrained('facebook/bart-large-cnn')\n",
    "\n",
    "# Loading the dataset of keywords\n",
    "datapath = r\"dataset.csv\"\n",
    "df = pd.read_csv(datapath)\n",
    "# Flatten the dataset to create a set of valid keywords\n",
    "valid_keywords = set()\n",
    "for column in df.columns:\n",
    "    valid_keywords.update(df[column].dropna().str.strip().tolist())\n",
    "\n",
    "def extract_keywords_from_tokens(text, model, tokenizer, num_keywords=5):\n",
    "    # Tokenize input\n",
    "    inputs = tokenizer(text, return_tensors='pt')\n",
    "    \n",
    "    # Get embeddings\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        last_hidden_state = outputs.last_hidden_state\n",
    "\n",
    "    # Convert token IDs to tokens\n",
    "    tokens = tokenizer.convert_ids_to_tokens(inputs['input_ids'][0])\n",
    "    \n",
    "    # Get the [CLS] token's embedding\n",
    "    cls_embedding = last_hidden_state[:, 0, :].squeeze()\n",
    "    \n",
    "    # Calculate similarity between each token embedding and the [CLS] embedding\n",
    "    similarities = torch.matmul(last_hidden_state.squeeze(), cls_embedding)\n",
    "    \n",
    "    # Get the indices of the top-n tokens with the highest similarity\n",
    "    top_indices = similarities.topk(num_keywords).indices\n",
    "\n",
    "    # Extract the corresponding tokens, excluding [CLS] and checking if they are in valid_keywords\n",
    "    keywords = [tokens[i] for i in top_indices if tokens[i] != '[CLS]' and tokens[i] in valid_keywords]\n",
    "    \n",
    "    return keywords\n",
    "\n",
    "# Read and process input text\n",
    "file_path = \"keywords.txt\"\n",
    "with open(file_path, \"r\") as file:\n",
    "    text = file.read()\n",
    "\n",
    "# Use the function to extract keywords\n",
    "keywords = extract_keywords_from_tokens(text, bert_model, bert_tokenizer)\n",
    "\n",
    "# Print extracted keywords\n",
    "print(\"Extracted keywords:\")\n",
    "for idx, keyword in enumerate(keywords, start=1):\n",
    "    print(f\"Keyword {idx}: {keyword}\")\n",
    "\n",
    "    # Search the web for the keyword on Wikipedia\n",
    "    search_url = f\"https://en.wikipedia.org/wiki/{keyword}\"\n",
    "    headers = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3\"}\n",
    "    response = requests.get(search_url, headers=headers)\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "    # Extract the relevant information from the search result\n",
    "    paragraphs = soup.find_all(\"p\")\n",
    "    extracted_text = \"\"\n",
    "    for paragraph in paragraphs:\n",
    "        extracted_text += paragraph.get_text() + \" \"\n",
    "    extracted_text = extracted_text.strip()\n",
    "\n",
    "    # Generate a summary using BART\n",
    "    def generate_summary(text, model, tokenizer):\n",
    "        inputs = tokenizer(text, return_tensors='pt', padding=True, truncation=True, max_length=1024)\n",
    "        summary_ids = model.generate(inputs['input_ids'], num_beams=4, max_length=150, early_stopping=True)\n",
    "        summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "        return summary\n",
    "\n",
    "    # Generate summary\n",
    "    summary = generate_summary(extracted_text, bart_model, bart_tokenizer)\n",
    "    print(f\"Summary of {keyword}:\")\n",
    "    print(summary)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This code uses pretrained data of the BERT to generate result so giving gibrish and non meaningful data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted keywords:\n",
      "Keyword 1: software\n",
      "Summary of software:\n",
      "what is software and how does it work? We look at some of the key features of software. What do you think? Let us know in the comments below. Back to Mail Online home. back to the page you came from.\"What is software?\" is a weekly, interactive look at software.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from transformers import BertTokenizer, BertModel, BartForConditionalGeneration, BartTokenizer\n",
    "\n",
    "# Load pre-trained BERT model and tokenizer\n",
    "bert_model = BertModel.from_pretrained('bert-base-uncased')\n",
    "bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Load pre-trained BART model and tokenizer for summarization\n",
    "bart_model = BartForConditionalGeneration.from_pretrained('facebook/bart-large-cnn')\n",
    "bart_tokenizer = BartTokenizer.from_pretrained('facebook/bart-large-cnn')\n",
    "\n",
    "# Loading the dataset of keywords\n",
    "datapath = r\"C:\\Users\\Lenovo\\Documents\\Rohit_AI_ML\\SummariseIT\\dataset.csv\"\n",
    "df = pd.read_csv(datapath)\n",
    "# Flatten the dataset to create a set of valid keywords\n",
    "valid_keywords = set()\n",
    "for column in df.columns:\n",
    "    valid_keywords.update(df[column].dropna().str.strip().tolist())\n",
    "\n",
    "def extract_keywords_from_tokens(text, model, tokenizer, num_keywords=5):\n",
    "    # Tokenize input\n",
    "    inputs = tokenizer(text, return_tensors='pt')\n",
    "    \n",
    "    # Get embeddings\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        last_hidden_state = outputs.last_hidden_state\n",
    "\n",
    "    # Convert token IDs to tokens\n",
    "    tokens = tokenizer.convert_ids_to_tokens(inputs['input_ids'][0])\n",
    "    \n",
    "    # Get the [CLS] token's embedding\n",
    "    cls_embedding = last_hidden_state[:, 0, :].squeeze()\n",
    "    \n",
    "    # Calculate similarity between each token embedding and the [CLS] embedding\n",
    "    similarities = torch.matmul(last_hidden_state.squeeze(), cls_embedding)\n",
    "    \n",
    "    # Get the indices of the top-n tokens with the highest similarity\n",
    "    top_indices = similarities.topk(num_keywords).indices\n",
    "\n",
    "    # Extract the corresponding tokens, excluding [CLS] and checking if they are in valid_keywords\n",
    "    keywords = [tokens[i] for i in top_indices if tokens[i] != '[CLS]' and tokens[i] in valid_keywords]\n",
    "    \n",
    "    return keywords\n",
    "\n",
    "def generate_summary(text, model, tokenizer):\n",
    "    inputs = tokenizer(text, return_tensors='pt', padding=True, truncation=True, max_length=1024)\n",
    "    summary_ids = model.generate(inputs['input_ids'], num_beams=4, max_length=150, early_stopping=True)\n",
    "    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "    return summary\n",
    "\n",
    "# Read and process input text\n",
    "file_path = \"C:/Users/Lenovo/Documents/Rohit_AI_ML/SummariseIT/keywords.txt\"\n",
    "with open(file_path, \"r\") as file:\n",
    "    text = file.read()\n",
    "\n",
    "# Use the function to extract keywords\n",
    "keywords = extract_keywords_from_tokens(text, bert_model, bert_tokenizer)\n",
    "\n",
    "# Print extracted keywords\n",
    "print(\"Extracted keywords:\")\n",
    "for idx, keyword in enumerate(keywords, start=1):\n",
    "    print(f\"Keyword {idx}: {keyword}\")\n",
    "\n",
    "    # Generate summary using the extracted keywords\n",
    "    summary = generate_summary(f\"what is {keyword}\", bart_model, bart_tokenizer)\n",
    "    print(f\"Summary of {keyword}:\")\n",
    "    print(summary)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
